const Grok = require("groq-sdk").default;

const groq = new Grok({
  apiKey: process.env.GROQ_API_KEY,
});

const generateJsonFromLLM = async ({ prompt, maxTokens }) => {

  console.log("\n\n\n\n  --> reaching :  backend/services/course.generate.service.js/generateJsonFromLLM . \n\n\n");

  console.log("\n\n\n\n outline prompt \n\n ", prompt, "\n\n\n");

  const response = await groq.chat.completions.create({
    model: "llama-3.1-8b-instant",
    messages: [
      { role: "system", content: "You are a strict JSON generator. Follow instructions exactly." },
      { role: "user", content: prompt }
    ],
    temperature: 0.3,
    max_tokens: maxTokens,
    response_format: { type: "json_object" },
  });

  const raw = response?.choices?.[0]?.message?.content;
  if (!raw) {
    console.log("\n\n\n\n error comes from backend/services/course.generate.service.js/generateJsonFromLLM and line no. 25 \n\n\n");
    throw new Error("Empty response from LLM");
  }

  console.log("\n\n\n\n &&&&&&&&&&&&&&&&&&3\n\n\n\n\n");

  try {
    return JSON.parse(raw.trim());
  } catch (err) {
   console.log("\n\n\n\n error comes from backend/services/course.generate.service.js/generateJsonFromLLM and line no. 34 \n\n\n");
    console.error("RAW LLM OUTPUT:", raw);
    throw new Error("Invalid JSON generated by LLM");
  }
};


const generateTopicAndDesciptionService = async ({ prompt }) => {
  console.log("\n\n\n\n  --> reaching :  backend/services/course.generate.service.js . \n\n\n");

  return generateJsonFromLLM({ prompt, maxTokens: 1000 });
};

const generateOutlineService = async ({ prompt }) => {
  console.log("\n\n\n\n  --> reaching :  backend/services/course.generate.service.js/generateOutlineService : printing  . \n\n\n", prompt, "\n\n\n");
  return generateJsonFromLLM({ prompt, maxTokens: 5000 });
};


const generateLessonService = async (prompt) => {
  console.log("\n\n\n\n  --> reaching :  backend/services/course.generate.service.js . \n\n\n");
  return generateJsonFromLLM({ prompt, maxTokens: 8000 });
};


const generateYouTubeQueryService = async (prompt) => {
  console.log("\n\n\n\n  --> reaching :  backend/services/course.generate.service.js . \n\n\n");

  console.log(`\n\n\n\n reaching ${__filename} \n\n\n\n`);


  return generateJsonFromLLM({ prompt, maxTokens: 3000 });
};


module.exports = { generateOutlineService, generateLessonService, generateTopicAndDesciptionService, generateYouTubeQueryService };